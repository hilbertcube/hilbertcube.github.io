<!DOCTYPE html>
<html lang="en">

<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-00W722FWP2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-00W722FWP2');
  </script>
  <title>The Big O Notation</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta name="subject" content='here'>
  <meta name="description" content="here" />
  <meta name="keywords" content="data structures, algorithms, c++, cpp, computer science" />
  <meta name="author" content="Don Le, ledongduu@gmail.com" />
  <meta name="format-detection" content="telephone=no"> <!-- Disable number string recognition-->
  <link rel="stylesheet" href="../../assets/css/main.css">
  <style>
    .front-img {
      display: block;
      width: 100%;
      border-radius: 10px;
      height: auto;
      margin: auto;
    }

    .algorithm_box {
      width: 50%;
    }

    @media all and (max-width: 580px) {
      .algorithm_box {
        width: 100%;
      }
    }

    #nlogn>.MathJax {
      font-size: inherit !important;
    }
  </style>
</head>

<body class="nav-open light-mode">
  <div class="mathjax-definition">
    \[
    \newcommand{\lbrac}{\left(}
    \newcommand{\rbrac}{\right)}
    \]
  </div>
  <nav class="navbar open" id="navbar">
    <div class="logo-and-side-nav"></div>
    <div class="toc">
      <header class="major">
        <h2>Table of Contents</h2>
      </header> 
      <ul>
        <li><a href="#introduction">Introduction</a>
          <ul>
            <li><a href="#compare-efficiency">What is the Best Way to Compare Algorithms' Efficiency?</a></li>
            <li><a href="#big-o-omega-theta">The Big O, Big Omega, Big Theta, and Their Properties</a>
              <ul>
                <li><a href="#big-o">The Big O</a></li>
                <li><a href="#big-omega">The Big Omega</a></li>
                <li><a href="#big-theta">The Big Theta</a></li>
                <li><a href="#useful-properties">Useful Properties</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#runtime-analysis">Runtime Analysis of an Algorithm</a>
          <ul>
            <li><a href="#master-theorem">The Master Theorem</a></li>
            <li><a href="#example-merge-sort">Example 1: Merge Sort</a></li>
            <li><a href="#example-binary-search">Example 2: Binary Search</a></li>
            <li><a href="#example-topological-sort">Example 3: Topological Sort</a></li>
            <li><a href="#example-dijkstra">Example 4: Dijkstra's Algorithm</a></li>
          </ul>
        </li>
        <li><a href="#fundamental-functions">The Fundamental Functions of Algorithmic Analysis</a>
          <ul>
            <li><a href="#constant-function">The Constant Function</a></li>
            <li><a href="#logarithm-function">The Logarithm Function</a></li>
            <li><a href="#linear-function">The Linear Function</a></li>
            <li><a href="#nlogn-function">The n log n Function</a></li>
            <li><a href="#quadratic-function">The Quadratic Function</a></li>
            <li><a href="#cubic-function">The Cubic Function</a></li>
            <li><a href="#exponential-function">The Exponential Function</a></li>
            <li><a href="#factorial-function">The Factorial Function</a></li>
            <li><a href="#irregular-functions">Other Irregular Functions</a></li>
          </ul>
        </li>
        <!-- <li><a href="#when-complexity-matters">When Does Time Complexity Start to Matter?</a></li>
        <li><a href="#algorithmic-analysis">Algorithmic Analysis</a></li> -->
        <li><a href="#references">References</a></li>
      </ul>
    </div>
    <div class="highlights-and-attribute"></div>
  </nav>

  <div class="general-wrapper">
    <div class="content-grid">

      <header>
        <div class="topic">
          Topics: Computer Science, Data Structures, Algorithms, C++
        </div>
        <h1 class="title">Algorithmic Analysis of an Algorithm</h1>
        <div class="date"></div>
        <figure style="margin: 0;">
          <img class="front-img" src="time-complexity.webp" alt="image">
        </figure>
        <div class="Quote">
          <div class="Quote-content">&#8220Science is what we understand well enough to explain to a computer. Art is
            everything else we do.&#8221</div>
          <div class="Author">- Donald Knuth</div>
        </div>
      </header>

      <section id="introduction">
        <h2>Introduction</h2>

        <section id="compare-efficiency">
          <h3>What is the Best Way to Compare Algorithms' Efficiency?</h3>
          <p>
            Given two algorithms that perform the same task, what is the best way to compare their performance? At first
            glance, one might consider measuring the runtime of the two algorithms. However, this is not a reliable
            metric, as runtime depends heavily on the hardware and can be highly inconsistent due to real-world
            conditions, even on the same machine. To address this problem, scientists have developed a more theoretical
            approach called the time complexity of an algorithm. This measure evaluates how an algorithm's performance
            scales as the size of the input increases.
          </p>
          <p>
            If $n$ is the number of input, then for a given algorithm, there exists a function $f(n)$ that measures the
            worst-case runtime of the algorithm as $n$ varies. The runtime of an algorithm generally follows a specific
            trend, such as constant, logarithmic, or linear. However, as noted, the runtime is significantly influenced
            by hardware and physical conditions. Therefore, these functions are often expressed with additional constant
            factors, such as:
          </p>
          <div class="equation">
            $$f(n) = C_1n + C_0,$$
          </div>
          <p>
            where $C_1$ and $C_0$ are factors that depends on outside conditions. This expression is quite inconvenient,
            as we don't usually know what these constant factors are, so the general rule of thumb to express the
            worst-case runtime of an algorithm is through the big-O notation. For example, if
            $f(n) = C_1n + C_0$, then it can be said that
          </p>
          <div class="equation">
            $$f(n) \in O(n).$$
          </div>
          <p>
            There are many categories of such functions $f(n)$, and the most important functions used in algorithmic
            analysis are the constant function, $f(n)=c$, the logarithm function $f(n) = \log_b(n)$, the linear function
            $f(n) = n$, $f(n) = n\log(n)$, the quadratic function $f(n) = n^2$, other polynomial functions $f(n) = n^d$,
            the exponential functions $f(n) = a^n$, and the factorial function $f(n) = n!$.
          </p>
        </section>

        <section id="big-o-omega-theta">
          <h3>The Big O, Big Omega, Big Theta, and Their Properties</h3>
            <p>
              Merge sort splits the input array into halves, recursively sorts both halves, then merges two sorted arrays in linear time.
              Let $T(n)$ be the time to sort $n$ items. The recurrence is
            </p>
            <div class="equation">$$T(n) = 2\,T\!\left(\tfrac{n}{2}\right) + \Theta(n).$$</div>
            <p>
              Here $a=2$, $b=2$, and $f(n)=\Theta(n)$ matches Case 2 of the Master Theorem with $k=0,\;\log_b a=1$, yielding $T(n) \in \Theta(n\,\log n)$. The merge step uses linear extra space, so space complexity is $\Theta(n)$.
            </p>

          <section id="big-o">
            <h4>The Big O Notation</h4>
            <div class="definition" data-definition-name=". The Big O">
              Let $f$ and $g$ be real-valued and not necessarily continuous functions. We say that $f$ is the big O of
              $g$, written
              <div class="equation">
                $$f(n) = O(g(n)),\quad n \to \infty,$$
              </div>
              if there exists some value $n_0$ such that
              <div class="equation">
                $$|f(n)| \le M|g(n)|,\quad \forall n \ge n_0.$$
              </div>
              for some constant $M > 0$.
            </div>

            <p>
              For example, $f(n) = 3n \in O(n)$ because $|3n| \le M|n|$ for $n \ge 0$, and $M$ can be any number greater than or equal to 3.
              Similarly, $8n + 5 \in O(n)$ because choosing $M = 9$ and $n_0 = 5$ gives $8n + 5 \le 9n$ for all $n \ge 5$.
            </p>
          </section>

            <p>
              Binary search on a sorted array halves the search interval each iteration. The recurrence is
            </p>
            <div class="equation">$$T(n) = T\!\left(\tfrac{n}{2}\right) + \Theta(1).$$</div>
            <p>
              With $a=1$, $b=2$, $f(n)=\Theta(1)$, we have $\log_b a = 0$ and Case 2 gives $T(n) \in \Theta(\log n)$. Iterative binary search uses $\Theta(1)$ extra space; the naive recursive form uses $\Theta(\log n)$ stack space.
            </p>
          <section id="big-omega">
            <h4>The Big Omega Notation</h4>
            <div class="definition" data-definition-name=". The Big Omega">
              Big Omega provides a lower bound on the growth of a function. We say that $f$ is the big Omega of $g$, written
              <div class="equation">
                $$f(n) = \Omega(g(n)),\quad n \to \infty,$$
              </div>
              if there exists some value $n_0$ such that
              <div class="equation">
                $$|f(n)| \ge M|g(n)|,\quad \forall n \ge n_0,$$
              </div>
              for some constant $M > 0$.
            </div>

            <p>
              For example, $5n \in \Omega(n)$ since $|5n| \ge M|n|$ with $M = 5$ for all $n \ge 0$.
              Likewise, $2n^2 + 7n \in \Omega(n^2)$ because for large enough $n$, the quadratic term dominates.
            </p>
          </section>

            <p>
              Using Kahn’s algorithm (indegree + queue) or DFS, each vertex is enqueued/visited a constant number of times and each edge is examined once.
              For a DAG with $V$ vertices and $E$ edges, the runtime is $\Theta(V+E)$; space is $\Theta(V)$ for the queue/stack and $\Theta(V+E)$ for adjacency lists.
            </p>
          <section id="big-theta">
            <h4>The Big Theta Notation</h4>
            <div class="definition" data-definition-name=". The Big Theta">
              Big Theta describes a tight bound on the growth of a function. We say that $f$ is the big Theta of $g$, written
              <div class="equation">
                $$f(n) = \Theta(g(n)),\quad n \to \infty,$$
              </div>
              if there exist constants $M_1, M_2 > 0$ and a value $n_0$ such that
              <div class="equation">
                $$M_1|g(n)| \le |f(n)| \le M_2|g(n)|,\quad \forall n \ge n_0.$$
              </div>
            </div>

            <p>
              For example, $7n + 4 \in \Theta(n)$ because the linear term dominates and both an upper bound and a lower bound
              proportional to $n$ can be found. Similarly, $3n^3 - 2n \in \Theta(n^3)$ because the cubic term determines the
              growth rate.
            </p>
          </section>

            <p>
              With adjacency lists and a binary heap priority queue:
            </p>
            <ul style="line-height: 1.6;">
              <li style="margin: var(--in-page-li-spacing) 0;">Extract-min occurs $V$ times at $\Theta(\log V)$ each.</li>
              <li style="margin: var(--in-page-li-spacing) 0;">Decrease-key (relaxation) occurs up to $E$ times at $\Theta(\log V)$ each.</li>
              <li style="margin: var(--in-page-li-spacing) 0;">Scanning adjacency lists costs $\Theta(E)$ overall.</li>
            </ul>
            <p>
              Total time $\Theta\big((V+E)\log V\big)$; with an array as the queue it is $\Theta(V^2+E)$; with a Fibonacci heap it becomes $\Theta(E+V\log V)$.
            </p>
          <section id="useful-properties">
            <h4>Useful Properties</h4>
            <ul style="line-height: 1.6;">
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Ignore constants:</b> $c\,f(n) \in \Theta(f(n))$ for any constant $c>0$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Base of logs:</b> $\log_a n \in \Theta(\log_b n)$ for any fixed bases $a,b>1$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Sum dominated by max:</b> $f(n)+g(n) \in \Theta(\max\{f(n),g(n)\})$ if one eventually dominates.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Products:</b> If $f\in O(g)$ and $h\in O(k)$, then $fh \in O(gk)$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Polynomial vs. exponential:</b> For any constant $d>0$ and any $a>1$, $n^d \in o(a^n)$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Transitivity:</b> If $f\in O(g)$ and $g\in O(h)$ then $f\in O(h)$ (similarly for $\Omega$).</li>
            </ul>
          </section>

          <section id="master-theorem">
            <h4>The Master Theorem</h4>
            <p>
              Many divide-and-conquer algorithms lead to recurrences of the form
            </p>
            <div class="equation">$$T(n) = a\,T\!\left(\frac{n}{b}\right) + f(n), \quad a \ge 1,\; b>1,$$</div>
            <p>
              where $a$ subproblems of size $n/b$ are solved and $f(n)$ accounts for splitting/merging work. The Master Theorem gives asymptotic solutions:
            </p>
            <ul style="line-height: 1.6;">
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Case 1:</b> If $f(n) \in O\big(n^{\log_b a - \varepsilon}\big)$ for some $\varepsilon>0$, then $T(n) \in \Theta\big(n^{\log_b a}\big)$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Case 2:</b> If $f(n) = \Theta\big(n^{\log_b a}\,\log^k n\big)$ for some $k \ge 0$, then $T(n) \in \Theta\big(n^{\log_b a}\,\log^{k+1} n\big)$.</li>
              <li style="margin: var(--in-page-li-spacing) 0;"><b>Case 3:</b> If $f(n) \in \Omega\big(n^{\log_b a + \varepsilon}\big)$ for some $\varepsilon>0$ and $a\,f(n/b) \le c\,f(n)$ for some constant $c<1$ and all sufficiently large $n$, then $T(n) \in \Theta\big(f(n)\big)$.</li>
            </ul>
          </section>
        </section>




      <section id="runtime-analysis">
        <h2>Runtime Analysis of an Algorithm</h2>
        <section id="example-merge-sort">
          <h3>Example 1: Merge Sort</h3>
          <p>
            Merge sort splits the input array into halves, recursively sorts both halves, then merges two sorted arrays in linear time.
            Let $T(n)$ be the time to sort $n$ items. The recurrence is
          </p>
          <div class="equation">$$T(n) = 2\,T\!\left(\tfrac{n}{2}\right) + \Theta(n),$$</div>
          <p>
            which solves to $T(n) \in \Theta(n\,\log n)$ by the Master Theorem. The merge step uses linear extra space, so space complexity is $\Theta(n)$.
          </p>
        </section>

        <section id="example-binary-search">
          <h3>Example 2: Binary Search</h3>
          <p>
            Binary search on a sorted array halves the search interval each iteration. The recurrence is
          </p>
          <div class="equation">$$T(n) = T\!\left(\tfrac{n}{2}\right) + \Theta(1) \;\Rightarrow\; T(n) \in \Theta(\log n).$$</div>
          <p>
            Iterative binary search uses $\Theta(1)$ extra space; the naive recursive form uses $\Theta(\log n)$ stack space.
          </p>
        </section>

        <section id="example-topological-sort">
          <h3>Example 3: Topological Sort</h3>
          <p>
            Kahn’s algorithm or a DFS-based algorithm visits each vertex and edge a constant number of times.
            For a directed acyclic graph (DAG) with $V$ vertices and $E$ edges, the runtime is $\Theta(V+E)$.
          </p>
        </section>

        <section id="example-dijkstra">
          <h3>Example 4: Dijkstra's Algorithm</h3>
          <p>
            With a binary heap (priority queue) and adjacency lists, Dijkstra runs in $\Theta\big((V+E)\log V\big)$.
            Using an array as the queue yields $\Theta(V^2+E)$; with a Fibonacci heap it becomes $\Theta(E+V\log V)$.
          </p>
        </section>
      </section>


      <section id="fundamental-functions">
        <h2>The Fundamental Functions of Algorithmic Analysis</h2>

        <section id="constant-function">
          <h3>The constant function</h3>
          <p>
            The constant time complexity function, denoted
          </p>
          <div class="equation">
            $$f(n) = c \in O(1),$$
          </div>
          <p>
            does not depend on the size of the input (unless the size of the input is exceptionally large). Some
            examples of algorithms with this complexity are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">Accessing array index: Accessing elements at a specific
              index in an array takes constant time because an array begins at a fixed memory location. The memory
              location of each element can be directly calculated using the element's size and its index, which is a
              constant operation.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              <p>
                Basic arithmetic and bitwise operations: Computers usually uses 8-bit, 16-bit, 32-bit, and 64-bit for
                integers and floating point numbers. Addition, subtraction, multiplication, or division of two numbers
                are considered constant time complexity because they are limited to a certain amount of bit. However,
                for large number arithmetic with arbitrary precision, the time complexity will no longer be constant.
              </p>
              <p>
                Bitwise operations like AND, OR, and XOR also take constant time because, again, primitive data types
                are limited to a certain size on the computer.
              </p>
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Variable assignment: For primitive data types such as
              integers, floating points, characters, and boolean, the assignment operator takes constant time complexity
              because the size of these variables are pre-determined. However, for non-primitive data types such as
              strings, the assignment operator takes linear time, $O(n)$ because strings are arrays of characters.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Returning a value from a function: Returning a pre-defined
              value also takes constant time.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Inserting a node, or jump to the next/previous node in a linked list: These operation are constant because
              each node holds the address of the next node (for a singly linked list), or the address of both the
              previous and the next node (for a doubly linked list).
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Pushing and popping on stack. Insertion and removal from queue.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Hashmap operations: Insert, access, and search operations
              in a hash map take constant time on average (ideally). These operations are $O(1)$ on average because a
              hash map uses a bucket array, and accessing elements in an array is $O(1)$. The hash function computes an
              index for a given key, and ideally, keys are distributed evenly across the buckets. However, if all keys
              end up in the same bucket due to hash collisions (e.g., in separate chaining), these operations degrade to
              $O(n)$ in the worst case, where $n$ is the total number of keys in the hash map.

            </li>
            </ul>
        </section>


        <section id="logarithm-function">
          <h3>The Logarithm Function</h3>
          <p>
            Some examples of this type of algorithms are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">Binary search: binary search involves searching for an
              element in a sorted array. This algorithm is $O(\log n)$ because it utilizes the divide and conquer
              technique, where the size of the array reduces by half every iteration.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Calculate the 64-bit (or less) $n$-th Fibonacci number with
              <a href="https://www.geeksforgeeks.org/fast-doubling-method-to-find-the-nth-fibonacci-number/"
                class="url">fast doubling method</a>.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Balanced Binary Search Trees (e.g., AVL Tree, Red-Black
              Tree): Searching, insertion, and deletion in balanced binary search trees like AVL trees, Red-Black trees,
              or B-trees operate in $O(\log n)$ because the height of a balanced tree is $O(\log n)$.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Binary Heap Operations: Operations like insertion and
              extracting the minimum (or maximum) in a binary heap take $O(\log n)$ time because of heapify operations.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Exponentiation by Squaring: This algorithm utilizes the
              divide and conquer method by checking whether $n$ is even or odd.
              <div style="text-align: center; margin: 25px auto;" class="algorithm_box">
                <pre class="console">
    <samp>Input: a real number x; an integer n
    Output: x^n
    exp_by_squaring(x, n)
        if n < 0 then
            return exp_by_squaring(1 / x, -n);
        else if n = 0 then 
            return 1;
        else if n is even then 
            return exp_by_squaring(x * x, n / 2);
        else if n is odd then 
            return x * exp_by_squaring(x * x, (n - 1) / 2);
    end function </samp></pre>
              </div>
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Finding the Greatest Common Divisor (GCD)
            </li>
          </ul>
        </section>



        <section id="linear-function">
          <h3>The Linear Function</h3>


          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Finding the maximum/minimum of an array: We can only find the maximum or the minimum of an array by going
              over all elements of the array.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Linear search: If the elements are not sorted, then to find an element in an array, we must perform linear
              search, which has the worst-case complexity of $O(n)$. Similarly, traversing a linked list is also $O(n)$.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Deletion and Insertion at random position in a linked list or array. This tasks requires us to traverse
              over each individual elements/nodes, which has the worst-case complexity of $O(n)$.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Comparing arrays, linked list, strings.
            </li>
          </ul>
        </section>


        <section id="nlogn-function">
          <h3>The <span id="nlogn">$n \log n$</span> Function</h3>
          <p>
            These types of algorithms are usually optimized versions of $O(n^2)$ algorithms. Some examples of this type
            of algorithms are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Fast Fourier Transform: FFT works by breaking down the computation of the Discrete Fourier Transform
              (DFT), which is $O(n^2)$ into smaller subproblems. For an input signal of size $n$, the FFT algorithm
              splits it into two halves: one for the even-indexed terms and one for the odd-indexed terms. It then
               recursively computes the DFT of these smaller subproblems: the even and odd indexed terms of the DFT.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Merge Sort, Quick Sort (on average), Heap Sort, AVL Tree Sort are all $O(n\;\log n)$ algorithms.
            </li>
          </ul>
        </section>

        <section id="quadratic-function">
          <h3>The Quadratic Function</h3>
          <p>
            Quadratic time $\Theta(n^2)$ typically arises from a constant number of nested loops over the input.
            Examples include selection sort, bubble sort, and insertion sort (worst case), naive substring search,
            and computing all pairwise distances in an array.
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">Two nested loops over $n$ elements: $\sum_{i=1}^{n} \sum_{j=1}^{n} 1 \in \Theta(n^2)$.</li>
            <li style="margin: var(--in-page-li-spacing) 0;">Insertion sort: $\Theta(n^2)$ in the worst/average case, $\Theta(n)$ in the best (already sorted).</li>
          </ul>
        </section>

        <section id="cubic-function">
          <h3>The Cubic Function</h3>
          <p>
            Cubic time $\Theta(n^3)$ appears in triple-nested loops and classical algorithms such as naive
            matrix multiplication. Faster asymptotic algorithms exist (e.g., Strassen’s $\Theta(n^{\log_2 7})$),
            but with large constants and practical trade-offs.
          </p>
        </section>


        <section id="exponential-function">
          <h3>The Exponential Function</h3>
          <p>
            Algorithms of this type are generally very bad. Some examples are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Subset Sum Problem (Brute Force): Given a set of integers, determine if there is a subset whose sum equals
              a given target value. A brute-force approach would involve checking every possible subset of the set to
              see if its sum equals the target. Time Complexity: $O(2^n)$ because there are
              <div class="equation">
                $$S = \sum_{k=0}^{n}\binom{n}{k} = \binom{n}{1} + \binom{n}{2} + \ldots + \binom{n}{n} = 2^n.$$
              </div>
              subsets of a set of $n$ elements.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Naive Recursive Fibonacci: Compute the $n$-th Fibonacci number, where $F(n) = F(n-1) + F(n-2)$ and the base
              cases are $F(0) = 0$ and $F(1) = 1$. The time complexity of this problem is $O(2^n)$ because the recursive
              calls grow exponentially due to repeated subproblems.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Find all sub-arrays of an array. This algorithm actually has a time complexity of $O(n\cdot 2^n)$ because
              the total number of non-empty sub-arrays in an array of size $n$ is $2^n - 1$ and we also need to loop
              over $n$ elements of the array.
            </li>
          </ul>
        </section>

        <section id="factorial-function">
          <h3>The Factorial Function</h3>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              The Traveling Salesman Problem (TSP) - Brute Force Approach: Given a set of cities, find the shortest
              possible route that visits each city exactly once and returns to the origin city. A brute force approach
              checks all possible permutations of the cities to find the optimal solution.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Generating Factorials: Given a set of integers $S=\{1, 2, \ldots, n\}$. Since there are up to $n!$
              permutations of elements of this set, generating all possible permutations will require $O(n!)$.
            </li>
          </ul>
        </section>

        <section id="irregular-functions">
          <h3>Other Irregular Functions</h3>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;"><b>Square-root time:</b> $\Theta(\sqrt{n})$ or $\Theta(n\sqrt{n})$ appears in number-theoretic sieves and some block-decomposition tricks.</li>
            <li style="margin: var(--in-page-li-spacing) 0;"><b>Polylogarithmic:</b> $\Theta(\log^k n)$ for fixed $k$ occurs in balanced trees and multi-level indexing.</li>
            <li style="margin: var(--in-page-li-spacing) 0;"><b>Iterated log:</b> $\log\log n$ and $\log^{(*)} n$ arise in specialized data structures.</li>
            <li style="margin: var(--in-page-li-spacing) 0;"><b>Inverse Ackermann:</b> $\alpha(n)$ (essentially a constant <em>in practice</em>) appears in union-find with path compression and union by rank; overall operations can be $\Theta(m\,\alpha(n))$ for $m$ operations on $n$ elements.</li>
          </ul>
        </section>
      </section>

<!-- 
      <section id="when-complexity-matters">
        <h2>When Does Time Complexity Start to Matter?</h2>
        <p>
          On tiny inputs, a well-tuned $\Theta(n^2)$ routine can beat a fancy $\Theta(n\log n)$ one. Past a certain size, the curve wins.
          At $n=10^5$, $n^2 \approx 10^{10}$ operations; $n\log_2 n \approx 1.7\times10^6$ — roughly <em>six thousand times</em> less work.
          That’s where asymptotics start paying rent.
        </p>
        <ul style="line-height: 1.6;">
          <li style="margin: var(--in-page-li-spacing) 0;"><b>First fix the class:</b> Move $\Theta(n^2)$ to $\Theta(n\log n)$ before micro-optimizing.</li>
          <li style="margin: var(--in-page-li-spacing) 0;"><b>Know your data:</b> Nearly-sorted inputs, sparsity, or bounded ranges can slash work.</li>
          <li style="margin: var(--in-page-li-spacing) 0;"><b>Trade memory for speed:</b> Hashing and preprocessing often turn $\Theta(n)$ scans into $\Theta(1)$ average lookups.</li>
          <li style="margin: var(--in-page-li-spacing) 0;"><b>Use amortized bounds:</b> Dynamic arrays and union–find spread occasional expensive steps over many cheap ones.</li>
        </ul>
      </section>

      <section id="algorithmic-analysis">
        <h2>Algorithmic Analysis</h2>
        <p>
          We commonly reason about <b>worst-case</b>, <b>average-case</b>, and <b>best-case</b> time, as well as <b>space complexity</b>.
          Amortized analysis bounds the average cost per operation over a sequence. Choose the model of computation and input distribution
          carefully; complexity guarantees often assume specific data structures and cost models.
        </p>
      </section> -->

      <section id="references">
        <h2>References</h2>
        <ul style="line-height: 1.6;">
          <li style="margin: var(--in-page-li-spacing) 0;">T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, <em>Introduction to Algorithms</em> (CLRS).</li>
          <li style="margin: var(--in-page-li-spacing) 0;">R. Sedgewick and K. Wayne, <em>Algorithms</em>, 4th ed.</li>
          <li style="margin: var(--in-page-li-spacing) 0;">D. E. Knuth, <em>The Art of Computer Programming</em>.</li>
          <li style="margin: var(--in-page-li-spacing) 0;"><a class="url" href="https://cp-algorithms.com/">cp-algorithms.com</a> – Practical algorithm notes and proofs.</li>
          <li style="margin: var(--in-page-li-spacing) 0;"><a class="url" href="https://en.wikipedia.org/wiki/Big_O_notation">Wikipedia: Big-O notation</a> – Reference for definitions and properties.</li>
        </ul>
      </section>

      <section>
        <h2>More Articles</h2>
        <div id="rec-article-container"></div>
      </section>

    </div>
    <footer></footer>
  </div>

  <script src="../../assets/js/scripts.js"></script>
  <script src="../../assets/js/blogpage-setting.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
  <!-- use prism-c.min.js or prism-clike.min.js-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
</body>

</html>